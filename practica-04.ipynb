{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Pr치ctica 4 - Inteligencia Artificial\n\n### Grado Ingenier칤a Inform치tica Tecnolog칤as Inform치ticas - Curso 2021-22\n\n### T칠cnicas metaheur칤sticas para optimizaci칩n\n### Procesos de Decisi칩n de Markov\n\nJos칠 Luis Ru칤z Reina y comentado por tu chavalito pesao de confianza 游땙",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "En esta pr치ctica vamos a implementar algoritmos relacionados con Procesos de Decisi칩n de Markov. \n\nSupondremos que un Procesos de Decisi칩n de Markov (MDP, por sus siglas en ingl칠s) va a ser un objeto de la siguiente clase (o mejor dicho, de una subclase de la siguiente clase). ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class MDP(object):\n\n    \"\"\"La clase gen칠rica MDP tiene como m칠todos la funci칩n de recompensa R,\n    la funci칩n A que da la lista de acciones aplicables a un estado, y la\n    funci칩n T que implementa el modelo de transici칩n. Para cada estado y\n    acci칩n aplicable al estado, la funci칩n T devuelve una lista de pares\n    (ei,pi) que describe los posibles estados ei que se pueden obtener al\n    plical la acci칩n al estado, junto con la probabilidad pi de que esto\n    ocurra. El constructor de la clase recibe la lista de estados posibles y\n    el factor de descuento.\n\n    En esta clase gen칠rica, las funciones R, A y T aparecen sin definir. Un\n    MDP concreto va a ser un objeto de una subclase de esta clase MDP, en la\n    que se definir치n de manera concreta estas tres funciones\"\"\"  \n\n    def __init__(self,estados,descuento):\n\n        self.estados=estados\n        self.descuento=descuento\n\n    def R(self,estado):\n        pass\n\n    def A(self,estado):\n        pass\n        \n    def T(self,estado,accion):\n        pass",
      "metadata": {
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Consideramos el siguiente problema:\n\nA lo largo de su vida, una empresa pasa por situaciones muy distintas, que por simplificar resumiremos en que al inicio de cada campa침a puede estar rica o pobre, y ser conocida o desconocida.  Para ello puede decidir en cada momento o bien invertir en publicidad, o bien optar por no hacer publicidad. Estas dos acciones no tienen siempre un resultado fijo, aunque podemos describirlo de manera probabil칤stica:\n\n* Si la empresa es rica y conocida y no invierte en publicidad, seguir치 rica, pero existe un 50% de probabilidad de que se vuelva desconocida. Si gasta en publicidad, con toda seguridad seguir치 conocida pero pasar치 a ser pobre.  \n* Si la empresa es rica y desconocida y no gasta en publicidad, seguir치 desconocida, y existe un 50% de que se vuelva pobre. Si gasta en publicidad, se volver치 pobre, pero existe un 50% de probabilidades de que se vuelva conocida.\n* Si la empresa es pobre y conocida y no invierte en publicidad, pasar치 a ser pobre y desconocida con un 50% de probabilidad, y rica y conocida en caso contrario. Si gasta en publicidad, con toda seguridad seguir치 en la misma situaci칩n. \n* Si la empresa es pobre y desconocida, y no invierte en publicidad, seguir치 en la misma situaci칩n con toda seguridad. Si gasta en publicidad, seguir치 pobre, pero con un 50% de posibilidades pasar치 aser conocida. \n\nSupondremos que la recompensa en una campa침a en la que la empresa es rica es de 10, y de 0 en en las que sea pobre. El objetivo es conseguir la mayor recompesa acumulada a lo largo del tiempo, aunque penalizaremos las ganancias obtenidas en campa침as muy lejanas en el tiempo, introduciendo un factor de descuento de 0.9. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 1\n\nSe pide representar el problema como un proceso de decisi칩n de Markov, definiendo una clase Rica_y_Conocida, subclase de la clase MDP gen칠rica, cuyo constructor recibe como entrada 칰nicamente el factor de descuento, y en la que se definen de manera concreta los m칠todos R, A y T, seg칰n lo descrito. Para ello:\n* La recompensa la guardaremos en un diccionario con claves \"RC\", \"RD\", \"PC\" y \"PD\", donde \"R\" es Rica, \"P\" es pobre, \"C\" es conocida y \"D\" es desconocida. Los valor del diccionario son las recompensas asociadas.\n* Las acciones ser치n: \"No publicidad\" y \"Gastar en publicidad\"\n* La transici칩n tambi칠n ser치 un diccionario donde las claves son tuplas (Estado, acci칩n) y los valores son listas de pares (Nuevo_estado, probabilidad). Por ejemplo un par clave-valor del diccionario ser치 (\"RC\",\"No publicidad\"):[(\"RC\",0.5),(\"RD\",0.5)]\n",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": "Recordamos que el factor de descuento es 0.9, que es el que viene definido en el enunciado, adem치s que la recompensa para ser rica es 10 y por ser pobre es 0. \n\nTambi칠n sabemmos que R es rica, P es pobre, C es conocida y D desconocida. Por tanto tenemos:\n\n* RC es Rica y Conocida\n* RD es Rica y Desconocida\n* PC es Pobre y Conocida\n* PD es pobre y Desconocida",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Soluci칩n:\nclass Rica_y_Conocida(MDP):\n    \n    def __init__(self, factor_descuento = 0.9):\n        \n        # Definimos las recompensas por estado, si es rica 10 y si es pobre 0\n        self.DiccionarioRecompensa = {\"RC\": 10, \"RD\": 10, \"PC\": 0, \"PD\": 0}\n        self.DiccionarioTransicion = {\n            # Si la empresa no invierte en publicidad hay un 50% de que siga siendo desconocida y 50% de que se vuelva desconocida. Pero sigue siendo rica\n            (\"RC\", \"No publicidad\"): [(\"RC\", 0.5), (\"RD\", 0.5)],\n            # Si se gasta el dinero en publicidad seguro (100%) que se vuelve pobre pero sigue siendo conocida\n            (\"RC\", \"Gastar en publicidad\"): [(\"PC\", 1)],\n            # Si no gasta en publicidad seguir치 siendo desconocida pero con un 50% de que se vuelva pobre o 50% de seguir siendo rica\n            (\"RD\", \"No publicidad\"): [(\"RD\", 0.5), (\"PD\", 0.5)],\n            # Si gasta en publicidad se vuelve pobre seguro (100%) pero con un 50% de ser conocida o 50% de seguir siendo desconocida\n            (\"RD\", \"Gastar en publicidad\"): [(\"PD\", 0.5), (\"PC\", 0.5)],\n            # Si no invierte en publicidad hay un 50% de volverse pobre y desconocida o un 50% de volverse rica y seguir siendo conocida\n            (\"PC\", \"No publicidad\"): [(\"PD\", 0.5),(\"RC\", 0.5)],\n            # Si gasta en publicidad seguir치 siendo pobre y conocida (100%)\n            (\"PC\", \"Gastar en publicidad\"): [(\"PC\", 1)],\n            # Si es pobre y desconocida sin gastar en publicidad con toda posibilidad (100%) va a seguir siendo pobre y desconocida\n            (\"PD\", \"No publicidad\"): [(\"PD\", 1)],\n            # Si gasta en publicidad hay un 50% que siga igual o un 50% de que se vuelva pobre y conocida\n            (\"PD\", \"Gastar en publicidad\"): [(\"PD\", 0.5),(\"PC\", 0.5)]\n            \n        }\n        estados = [\"RC\", \"RD\", \"PC\", \"PD\"]\n        super().__init__(estados, factor_descuento)\n        \n    # Definimos las recompensas\n    def R(self,estado):\n        return self.DiccionarioRecompensa[estado]\n    \n    # Definimos las acciones\n    def A(self,estado):\n        return [\"No publicidad\", \"Gastar en publicidad\"]\n\n    # Definimos las transiciones\n    def T(self,estado,accion):\n        return self.DiccionarioTransicion[(estado,accion)]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 2\n\nEn general, dado un MDP, representaremos una pol칤tica para el mismo como un diccionario cuyas claves son los estados, y los valores las acciones. Una pol칤tica representa una manera concreta de decidir en cada estado la acci칩n (de entre las aplicables a ese estado) que ha de aplicarse. \n\nDado un MDP, un estado de partida, y una pol칤tica concreta, podemos generar (muestrear) una secuencia de estados por los que ir칤amos pasando si vamos aplicando las acciones que nos indica la pol칤tica: dado un estado de la secuencia, aplicamos a ese estado la acci칩n que indique la pol칤tica, y obtenemos un estado siguiente de manera aleatoria, pero siguiendo la distribuci칩n de probabilidad que indica el modelo de transici칩n dado por el m칠todo T.  \n\nSe pide definir una funci칩n \"genera_secuencia_estados(mdp,pi,e,n)\" que devuelva una secuencia de estados de longitud n, obtenida siguiendo el m칠todo anterior. Aqu칤 mdp es objeto de la clase MDP, pi es una pol칤tica, e un estado de partida y n la longitud de la secuencia. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Soluci칩n\n\ndef muestreo(estado):\n    # Definimos valor aleatorio que usaremos en el muestreo\n    valor_aleatorio = random.random()\n    # Inicializamos la variable que tendr치 la pol칤tica del estado que habr치 en el bucle actualmente\n    politica_actual = 0\n    # Bucle que recorre el estado con el par accion, politica\n    for accion,politica in estado:\n        # A침adimos la politica actual y vamos a침adi칠ndola a la variable\n        politica_actual += politica\n        # Si la pol칤tica actual es mayor que el valor aleatorio ...\n        if politica_actual > valor_aleatorio:\n            # ... devolvemos el valor de la pol칤tica actual\n            return politica_actual\n    \ndef genera_secuencia_estados(mdp,pi,e,n):\n    # Vamos a usar el estado e como entrada\n    actual = e\n    # La variable con la secuencia generada recoge el estado_actual\n    secuencia_generada = [actual]\n    # Hace el for iterando n-1 porque empezamos por el valor 0\n    for _ in range(n-1):\n        # Le pasamos el modelo mdp marcando la transici칩n con el estado y la pol칤tica del mismo\n        estado_actual = muestreo(mdp.T(actual, pi[actual]))\n        # A침adimos el estado del muestreo a la secuencia que queremos generar\n        secuencia_generada.append(actual)\n    return secuencia_generada",
      "metadata": {
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Vemos a continuaci칩n algunos ejemplo de uso:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Definimos una instancia de la subclase \nmdp_ryc=Rica_y_Conocida()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "pi_ryc_ahorra={\"RC\":\"No publicidad\",\"RD\":\"No publicidad\",\n                    \"PC\":\"No publicidad\",\"PD\":\"No publicidad\"}\ngenera_secuencia_estados(mdp_ryc,pi_ryc_ahorra,\"PC\",10)\n\n# Posible resultado:\n# ['PC', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD']",
      "metadata": {
        "trusted": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['PC', 'PC', 'PC', 'PC', 'PC', 'PC', 'PC', 'PC', 'PC', 'PC']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "pi_ryc_2={\"RC\":\"No publicidad\",\"RD\":\"Gastar en publicidad\",\n               \"PC\":\"No publicidad\",\"PD\":\"Gastar en publicidad\"}\ngenera_secuencia_estados(mdp_ryc,pi_ryc_2,\"RD\",8)\n\n# Posible resultado:\n# ['RD', 'PC', 'RC', 'RC', 'RC', 'RC', 'RD', 'PC']",
      "metadata": {
        "trusted": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['RD', 'RD', 'RD', 'RD', 'RD', 'RD', 'RD', 'RD']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 3\n\nDado un MDP y una secuencia de estados, valoramos dicha secuencia como la suma de las recompensas de los estados de la secuencias (cada una con el correspondiente descuento). Se pide definir una funci칩n valora_secuencia(seq,mdp) que calcula esta valoraci칩n.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Soluci칩n\n\ndef valora_secuencia(seq, mdp):\n    return sum(mdp.R(e)*(mdp.descuento**i) for i,e in enumerate(seq))\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "valora_secuencia(['PC', 'RC', 'RC', 'RC', 'RC', 'RC', \n                       'RD', 'RD', 'RD', 'PD', 'PD', 'PD', \n                       'PD', 'PD', 'PD', 'PD', 'PD', 'PD', \n                       'PD', 'PD'],mdp_ryc)\n\n# Resultado:\n# 51.2579511",
      "metadata": {
        "trusted": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "51.2579511"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "valora_secuencia(['RD', 'PC', 'PD', 'PC', 'RC', 'RC', \n                        'RD', 'PD', 'PD', 'PC', 'RC', 'RC', \n                        'RC', 'RC', 'RC', 'RC'],mdp_ryc)\n\n# Resultado:\n# 44.11795212148159",
      "metadata": {
        "trusted": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "44.11795212148159"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 4\n\nDada una pol칤tica pi, la valoraci칩n de un estado e respecto de esa pol칤tica, V^{pi}(e), se define como la media esperada de las valoraciones de las secuencias que se pueden generar teniendo dicho estado como estado de partida. Usando las funciones de los dos ejercicios anteriores, definir una funci칩n \"estima_valor(e,pi,mdp,m,n)\" que realiza una estimaci칩n aproximada del valor V^{pi}(e), para ello genera n secuencias de tama침o m, y calcula la media de sus valoraciones.  ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ya que vamos a calcular la media como soluci칩n principal usaremos el sum(...)/n. Siendo n el n칰mero de secuencias, y siendo estas las que se van a recorrer valor치ndolas. \n\nPara ello, como se comenta anteriormente usaremos la funcion de valora_secuencia(...) para valorar las secuencias n veces, como valor de entrada a la funci칩n le vamos a pasar secuencias que generaremos con la funci칩n genera_secuencia_estados(...) y pas치ndole el MDP como entrada.\n\nEn genera_secuencia_estados(...) tenemos al objeto mdp, a e que son los estados de la secuencia, pi que ser치 la pol칤tica usada y a m la longitud de los estados.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Soluci칩n:\ndef estima_valor(e,pi,mdp,m,n):\n    return(sum(valora_secuencia(genera_secuencia_estados(mdp,pi,e,m), mdp) for _ in range (n)))/ n\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"PC\",pi_ryc_ahorra,mdp_ryc,50,500)\n\n# Respuesta posible:\n# 14.531471247172597",
      "metadata": {
        "trusted": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"PC\",pi_ryc_2,mdp_ryc,50,500)\n\n# Respuesta posible:\n# 35.92126959549151",
      "metadata": {
        "trusted": true
      },
      "execution_count": 18,
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"RC\",pi_ryc_ahorra,mdp_ryc,60,700)\n\n# Respuesta posible:\n# 32.807558694112984",
      "metadata": {
        "trusted": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99.8202989700079"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"RC\",pi_ryc_2,mdp_ryc,60,700)\n\n# Respuesta posible:\n# 50.09728516585913",
      "metadata": {
        "trusted": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99.8202989700079"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 5\n\nUsando la funci칩n anterior, estimar la valoraci칩n de cada estado del problema \"Rica y conocida\", con las dos siguientes pol칤ticas:\n\n* Aquella que sea cual sea el estado, siempre decide invertir en publicidad. \n* Aquella que sea cual sea el estado, siempre decide ahorrar. \n\n쮺u치l crees que es mejor? 쮿abr치 alguna mejor que estas dos? 쮺u치l crees que ser칤a la mejor pol칤tica de todas? ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "La soluci칩n para este ejercicio no va a ser m치s que crear las variables que implemente las casu칤sticas que nos plantean.\n\nA su vez pondremos como nombre a las variables las que nos ofrecen los ejemplos de debajo de la soluci칩n para que no haya problemas al ejecutarlo.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Soluci칩n\n\n# Esta es la opci칩n en la que siempre se invierte en publicidad, refrescamos los estados y acciones en la definici칩n de RYC si lo necesitamos\npi_ryc_gastar = {\"RC\": \"Gastar en publicidad\",\n                \"RD\": \"Gastar en publicidad\",\n                \"PC\": \"Gastar en publicidad\",\n                \"PD\": \"Gastar en publicidad\"}\n\npi_ryc_ahorra = {\"RC\": \"No publicidad\",\n                \"RD\": \"No publicidad\",\n                \"PC\": \"No publicidad\",\n                \"PD\": \"No publicidad\"}",
      "metadata": {
        "trusted": true
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"RC\",pi_ryc_gastar,mdp_ryc,60,1000)\n\n# Respuesta: 10.0",
      "metadata": {
        "trusted": true
      },
      "execution_count": 30,
      "outputs": [
        {
          "execution_count": 30,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99.8202989700078"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"RC\",pi_ryc_ahorra,mdp_ryc,60,1000)\n\n# Respuesta: 33.354461818277635",
      "metadata": {
        "trusted": true
      },
      "execution_count": 31,
      "outputs": [
        {
          "execution_count": 31,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99.8202989700078"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"RD\",pi_ryc_gastar,mdp_ryc,60,1000)\n\n# Respuesta: 10.0",
      "metadata": {
        "trusted": true
      },
      "execution_count": 32,
      "outputs": [
        {
          "execution_count": 32,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99.8202989700078"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"RD\",pi_ryc_ahorra,mdp_ryc,60,1000)\n\n# Respuesta:18.17532275274187",
      "metadata": {
        "trusted": true
      },
      "execution_count": 33,
      "outputs": [
        {
          "execution_count": 33,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99.8202989700078"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"PC\",pi_ryc_gastar,mdp_ryc,60,1000)\n\n# Respuesta: 0.0",
      "metadata": {
        "trusted": true
      },
      "execution_count": 34,
      "outputs": [
        {
          "execution_count": 34,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"PC\",pi_ryc_ahorra,mdp_ryc,60,1000)\n\n# Respuesta: estima_valor(\"PC\",pi_ryc_ahorra,mdp_ryc,60,1000)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 35,
      "outputs": [
        {
          "execution_count": 35,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"PD\",pi_ryc_gastar,mdp_ryc,60,1000)\n\n# Respuesta: 0.0",
      "metadata": {
        "trusted": true
      },
      "execution_count": 36,
      "outputs": [
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Se puede comprobar que salvo en el 칰ltimo caso, que la valoraci칩n es igual, las valoraciones que se consiguen con la segunda pol칤tica son mayores que con la primera pol칤tica. \n\nNinguna de las dos pol칤ticas es la 칩ptima, como se ver치 m치s adelante.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 6\n\nLa funci칩n de valoraci칩n no se suele calcular mediante la t칠cnica de muestreo vista en el ejercicio 4, sino como resultado de resolver un sistema de ecuaciones. Dicho sistema de ecuaciones se puede resolver de manera proximada de manera iterativa, tal como se explica en el tema.\n\nDefinir una funci칩n \"valoraci칩n_respecto_pol칤tica(pi,mdp, n)\" que aplica dicho m칠todo iterativo (n iteraciones) para calcular la valoraci칩n V^{pi}. Dicha valoraci칩n debe devolverse como un diccionario que a cada estado e le asocia el valor \"V^{pi}(e)\" calculado.  \n\nAplicar la funci칩n para calcular la valoraci칩n asociada a las dos pol칤ticas que se dan en el ejercicio anterior, y comparara los valores obtenidos con los obtenidos mediante muestreo. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Soluci칩n:\n\ndef valoraci칩n_respecto_pol칤tica(pi,mdp,n):\n    \n    # Asignamos las variables que vamos a usar en la funci칩n con respecto a los valores de la clase MDP\n    recompensa, transicion, descuento = mdp.R, mdp.T, mdp.descuento\n    \n    # valoracion va a ser la variable diccionario al que que se le recorre todos los estados y se le asocia el valor calculado\n    valoracion = {e:0 for estado in mdp.estados}\n    \n    for _ in range(n):\n        # Hacemos una copia de valoracion para no modificarla\n        valoracion_copia = valoracion.copy()\n        for estados in mdp.estados:\n            valoracion[estados] = recompensa(estados) + descuento*(sum([pi*valoracion_copia[estados, pi] in transicion(estados, pi[estados])]))\n    \n    return valoracion\n    ",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Calculamos con esta funci칩n la valoraci칩n de las dos pol칤ticas anteriores.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "valoraci칩n_respecto_pol칤tica(pi_ryc_gastar,mdp_ryc, 100)\n\n# Resultado:\n# {'RC': 10.0, 'RD': 10.0, 'PC': 0.0, 'PD': 0.0}",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'pi_ryc_gastar' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m valoraci칩n_respecto_pol칤tica(\u001b[43mpi_ryc_gastar\u001b[49m,mdp_ryc, \u001b[38;5;241m100\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pi_ryc_gastar' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "valoraci칩n_respecto_pol칤tica(pi_ryc_ahorra,mdp_ryc, 100)\n\n# Resultado:\n# {'RC': 33.05785123966942,\n#  'RD': 18.18181818181818,\n#  'PC': 14.876033057851238,\n#  'PD': 0.0}",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'pi_ryc_ahorra' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m valoraci칩n_respecto_pol칤tica(\u001b[43mpi_ryc_ahorra\u001b[49m,mdp_ryc, \u001b[38;5;241m100\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pi_ryc_ahorra' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 7\n\nEn el tema 3 se ha visto que la valoraci칩n de un estado se define como la mejor valoraci칩n que pueda tener el estado, respecto a todas las pol칤ticas posibles. Y la pol칤tica 칩ptima es aquella que en cada estado realiza la acci칩n con la mejor valoraci칩n esperada (entendiendo por valoraci칩n esperada la suma de las valoraciones de los estados que podr칤an resultar al aplicar dicha acci칩n, ponderadas por la probabilidad de que ocurra eso). De esta manera, la valoraci칩n de un estado es la valoraci칩n que la pol칤tica 칩ptima asigna al estado.\n\nPara calcular tanto la valoraci칩n de los estados, como la pol칤tica 칩ptima, se han visto dos algoritmos: iteraci칩n de valores e iteraci칩n de pol칤ticas. En este ejercicio se pide implementar el algoritmo de iteraci칩n de pol칤ticas. En concreto, se pide definir una funci칩n \"iteraci칩n_de_pol칤ticas(mdp,k)\" que implementa el algoritmo de iteraci칩n de pol칤ticas, y devuelve dos diccionarios, uno con la valoraci칩n de los estados y otro con la pol칤tica 칩ptima. \n\nComparar los resultados obtenidos con las pol칤ticas del ejercicio 5 y las valoraciones obtenidas.  ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Vamos a definir la funci칩n que nos devuelva la acci칩n con la valoraci칩n m치xima recorriendo todos los valores que haya en la secuencia de entrada",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def secuencia_maxima(secuencia, funcion):\n    max = float(\"-inf\")\n    #Lo inicializamos a None para que sea un valor neutro\n    valor_max = None\n    for valor in secuencia:\n        funcion_valor = funcion(valor)\n        if funcion_valor > max:\n            max = funcion_valor\n            accion_max = valor\n    return accion_max",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def valoracion_esperada(accion,estado,V,mdp):\n    return sum((p*V[e] for (e,p) in mdp.T(estado,accion)))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Soluci칩n:\n# Le ponemos la tilde que luego el ejemplo sino da fallo\ndef iteraci칩n_de_pol칤ticas(mdp,k):\n    V = {e:0 for e in mdp.estados}\n    politica = {e:random.choice(mdp.A(e)) for e in mdp.estados}\n    \n    while True:\n        V = valoraci칩n_respecto_pol칤tica(politica, mdp, k)\n        actualizado = False\n        for e in mdp.estados:\n             accion = secuencia_maxima(mdp.A(e))\n            if(accion != politica[e] and a:valoracion_esperada(accion,estado,V,mdp) > valoracion_esperada(politica[e], e, V, mdp)):\n                politica[e] = accion\n                actualizado = True\n        if not actualizado:\n            return politica, V",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Calculamos la mejor pol칤tica y su valoraci칩n con el MDP de Rica_y_Conocida. Como se ve, resulta que lo mejor es s칩lo gastar en publicidad cuando se es pobre y desconocido. Se observa que la valoraci칩n respecto de esa pol칤tica es mejor, que las valoraciones con las pol칤tica que se vieron en los ejercicios 5 y 6.   ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "iteraci칩n_de_pol칤ticas(mdp_ryc,100)\n\n# Respuesta\n# ({'RC': 'No publicidad',\n#   'RD': 'No publicidad',\n#   'PC': 'No publicidad',\n#   'PD': 'Gastar en publicidad'},\n#  {'RC': 54.20053629623792,\n#   'RD': 44.02311379672535,\n#   'PC': 38.602953921506,\n#   'PD': 31.584041852876634})",
      "metadata": {
        "trusted": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'iteraci칩n_de_pol칤ticas' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43miteraci칩n_de_pol칤ticas\u001b[49m(mdp_ryc,\u001b[38;5;241m100\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'iteraci칩n_de_pol칤ticas' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
