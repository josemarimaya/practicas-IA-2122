{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Pr√°ctica 4 - Inteligencia Artificial\n\n### Grado Ingenier√≠a Inform√°tica Tecnolog√≠as Inform√°ticas - Curso 2021-22\n\n### T√©cnicas metaheur√≠sticas para optimizaci√≥n\n### Procesos de Decisi√≥n de Markov\n\nJos√© Luis Ru√≠z Reina y comentado por tu chavalito pesao de confianza üòé",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "En esta pr√°ctica vamos a implementar algoritmos relacionados con Procesos de Decisi√≥n de Markov. \n\nSupondremos que un Procesos de Decisi√≥n de Markov (MDP, por sus siglas en ingl√©s) va a ser un objeto de la siguiente clase (o mejor dicho, de una subclase de la siguiente clase). ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class MDP(object):\n\n    \"\"\"La clase gen√©rica MDP tiene como m√©todos la funci√≥n de recompensa R,\n    la funci√≥n A que da la lista de acciones aplicables a un estado, y la\n    funci√≥n T que implementa el modelo de transici√≥n. Para cada estado y\n    acci√≥n aplicable al estado, la funci√≥n T devuelve una lista de pares\n    (ei,pi) que describe los posibles estados ei que se pueden obtener al\n    plical la acci√≥n al estado, junto con la probabilidad pi de que esto\n    ocurra. El constructor de la clase recibe la lista de estados posibles y\n    el factor de descuento.\n\n    En esta clase gen√©rica, las funciones R, A y T aparecen sin definir. Un\n    MDP concreto va a ser un objeto de una subclase de esta clase MDP, en la\n    que se definir√°n de manera concreta estas tres funciones\"\"\"  \n\n    def __init__(self,estados,descuento):\n\n        self.estados=estados\n        self.descuento=descuento\n\n    def R(self,estado):\n        pass\n\n    def A(self,estado):\n        pass\n        \n    def T(self,estado,accion):\n        pass",
      "metadata": {
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Consideramos el siguiente problema:\n\nA lo largo de su vida, una empresa pasa por situaciones muy distintas, que por simplificar resumiremos en que al inicio de cada campa√±a puede estar rica o pobre, y ser conocida o desconocida.  Para ello puede decidir en cada momento o bien invertir en publicidad, o bien optar por no hacer publicidad. Estas dos acciones no tienen siempre un resultado fijo, aunque podemos describirlo de manera probabil√≠stica:\n\n* Si la empresa es rica y conocida y no invierte en publicidad, seguir√° rica, pero existe un 50% de probabilidad de que se vuelva desconocida. Si gasta en publicidad, con toda seguridad seguir√° conocida pero pasar√° a ser pobre.  \n* Si la empresa es rica y desconocida y no gasta en publicidad, seguir√° desconocida, y existe un 50% de que se vuelva pobre. Si gasta en publicidad, se volver√° pobre, pero existe un 50% de probabilidades de que se vuelva conocida.\n* Si la empresa es pobre y conocida y no invierte en publicidad, pasar√° a ser pobre y desconocida con un 50% de probabilidad, y rica y conocida en caso contrario. Si gasta en publicidad, con toda seguridad seguir√° en la misma situaci√≥n. \n* Si la empresa es pobre y desconocida, y no invierte en publicidad, seguir√° en la misma situaci√≥n con toda seguridad. Si gasta en publicidad, seguir√° pobre, pero con un 50% de posibilidades pasar√° aser conocida. \n\nSupondremos que la recompensa en una campa√±a en la que la empresa es rica es de 10, y de 0 en en las que sea pobre. El objetivo es conseguir la mayor recompesa acumulada a lo largo del tiempo, aunque penalizaremos las ganancias obtenidas en campa√±as muy lejanas en el tiempo, introduciendo un factor de descuento de 0.9. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 1\n\nSe pide representar el problema como un proceso de decisi√≥n de Markov, definiendo una clase Rica_y_Conocida, subclase de la clase MDP gen√©rica, cuyo constructor recibe como entrada √∫nicamente el factor de descuento, y en la que se definen de manera concreta los m√©todos R, A y T, seg√∫n lo descrito. Para ello:\n* La recompensa la guardaremos en un diccionario con claves \"RC\", \"RD\", \"PC\" y \"PD\", donde \"R\" es Rica, \"P\" es pobre, \"C\" es conocida y \"D\" es desconocida. Los valor del diccionario son las recompensas asociadas.\n* Las acciones ser√°n: \"No publicidad\" y \"Gastar en publicidad\"\n* La transici√≥n tambi√©n ser√° un diccionario donde las claves son tuplas (Estado, acci√≥n) y los valores son listas de pares (Nuevo_estado, probabilidad). Por ejemplo un par clave-valor del diccionario ser√° (\"RC\",\"No publicidad\"):[(\"RC\",0.5),(\"RD\",0.5)]\n",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": "Recordamos que el factor de descuento es 0.9, que es el que viene definido en el enunciado, adem√°s que la recompensa para ser rica es 10 y por ser pobre es 0. \n\nTambi√©n sabemmos que R es rica, P es pobre, C es conocida y D desconocida. Por tanto tenemos:\n\n* RC es Rica y Conocida\n* RD es Rica y Desconocida\n* PC es Pobre y Conocida\n* PD es pobre y Desconocida",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Soluci√≥n:\nclass Rica_y_Conocida(MDP):\n    \n    def __init__(self, factor_descuento = 0.9):\n        \n        # Definimos las recompensas por estado, si es rica 10 y si es pobre 0\n        self.DiccionarioRecompensa = {\"RC\": 10, \"RD\": 10, \"PC\": 0, \"PD\": 0}\n        self.DiccionarioTransicion = {\n            # Si la empresa no invierte en publicidad hay un 50% de que siga siendo desconocida y 50% de que se vuelva desconocida. Pero sigue siendo rica\n            (\"RC\", \"No publicidad\"): [(\"RC\", 0.5), (\"RD\", 0.5)],\n            # Si se gasta el dinero en publicidad seguro (100%) que se vuelve pobre pero sigue siendo conocida\n            (\"RC\", \"Gastar en publicidad\"): [(\"PC\", 1)],\n            # Si no gasta en publicidad seguir√° siendo desconocida pero con un 50% de que se vuelva pobre o 50% de seguir siendo rica\n            (\"RD\", \"No publicidad\"): [(\"RD\", 0.5), (\"PD\", 0.5)],\n            # Si gasta en publicidad se vuelve pobre seguro (100%) pero con un 50% de ser conocida o 50% de seguir siendo desconocida\n            (\"RD\", \"Gastar en publicidad\"): [(\"PD\", 0.5), (\"PC\", 0.5)],\n            # Si no invierte en publicidad hay un 50% de volverse pobre y desconocida o un 50% de volverse rica y seguir siendo conocida\n            (\"PC\", \"No publicidad\"): [(\"PD\", 0.5),(\"RC\", 0.5)],\n            # Si gasta en publicidad seguir√° siendo pobre y conocida (100%)\n            (\"PC\", \"Gastar en publicidad\"): [(\"PC\", 1)],\n            # Si es pobre y desconocida sin gastar en publicidad con toda posibilidad (100%) va a seguir siendo pobre y desconocida\n            (\"PD\", \"No publicidad\"): [(\"PD\", 1)],\n            # Si gasta en publicidad hay un 50% que siga igual o un 50% de que se vuelva pobre y conocida\n            (\"PD\", \"Gastar en publicidad\"): [(\"PD\", 0.5),(\"PC\", 0.5)]\n            \n        }\n        estados = [\"RC\", \"RD\", \"PC\", \"PD\"]\n        super().__init__(estados, factor_descuento)\n        \n    # Definimos las recompensas\n    def R(self,estado):\n        return self.DiccionarioRecompensa[estado]\n    \n    # Definimos las acciones\n    def A(self,estado):\n        return [\"No publicidad\", \"Gastar en publicidad\"]\n\n    # Definimos las transiciones\n    def T(self,estado,accion):\n        return self.DiccionarioTransicion[(estado,accion)]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 2\n\nEn general, dado un MDP, representaremos una pol√≠tica para el mismo como un diccionario cuyas claves son los estados, y los valores las acciones. Una pol√≠tica representa una manera concreta de decidir en cada estado la acci√≥n (de entre las aplicables a ese estado) que ha de aplicarse. \n\nDado un MDP, un estado de partida, y una pol√≠tica concreta, podemos generar (muestrear) una secuencia de estados por los que ir√≠amos pasando si vamos aplicando las acciones que nos indica la pol√≠tica: dado un estado de la secuencia, aplicamos a ese estado la acci√≥n que indique la pol√≠tica, y obtenemos un estado siguiente de manera aleatoria, pero siguiendo la distribuci√≥n de probabilidad que indica el modelo de transici√≥n dado por el m√©todo T.  \n\nSe pide definir una funci√≥n \"genera_secuencia_estados(mdp,pi,e,n)\" que devuelva una secuencia de estados de longitud n, obtenida siguiendo el m√©todo anterior. Aqu√≠ mdp es objeto de la clase MDP, pi es una pol√≠tica, e un estado de partida y n la longitud de la secuencia. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Soluci√≥n\n\ndef muestreo(estado):\n    # Definimos valor aleatorio que usaremos en el muestreo\n    valor_aleatorio = random.random()\n    # Inicializamos la variable que tendr√° la pol√≠tica del estado que habr√° en el bucle actualmente\n    politica_actual = 0\n    # Bucle que recorre el estado con el par accion, politica\n    for accion,politica in estado:\n        # A√±adimos la politica actual y vamos a√±adi√©ndola a la variable\n        politica_actual += politica\n        # Si la pol√≠tica actual es mayor que el valor aleatorio ...\n        if politica_actual > valor_aleatorio:\n            # ... devolvemos el valor de la pol√≠tica actual\n            return politica_actual\n    \ndef genera_secuencia_estados(mdp,pi,e,n):\n    # Vamos a usar el estado e como entrada\n    actual = e\n    # La variable con la secuencia generada recoge el estado_actual\n    secuencia_generada = [actual]\n    # Hace el for iterando n-1 porque empezamos por el valor 0\n    for _ in range(n-1):\n        # Le pasamos el modelo mdp marcando la transici√≥n con el estado y la pol√≠tica del mismo\n        estado_actual = muestreo(mdp.T(actual, pi[actual]))\n        # A√±adimos el estado del muestreo a la secuencia que queremos generar\n        secuencia_generada.append(actual)\n    return secuencia_generada",
      "metadata": {
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Vemos a continuaci√≥n algunos ejemplo de uso:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Definimos una instancia de la subclase \nmdp_ryc=Rica_y_Conocida()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "pi_ryc_ahorra={\"RC\":\"No publicidad\",\"RD\":\"No publicidad\",\n                    \"PC\":\"No publicidad\",\"PD\":\"No publicidad\"}\ngenera_secuencia_estados(mdp_ryc,pi_ryc_ahorra,\"PC\",10)\n\n# Posible resultado:\n# ['PC', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD']",
      "metadata": {
        "trusted": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['PC', 'PC', 'PC', 'PC', 'PC', 'PC', 'PC', 'PC', 'PC', 'PC']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "pi_ryc_2={\"RC\":\"No publicidad\",\"RD\":\"Gastar en publicidad\",\n               \"PC\":\"No publicidad\",\"PD\":\"Gastar en publicidad\"}\ngenera_secuencia_estados(mdp_ryc,pi_ryc_2,\"RD\",8)\n\n# Posible resultado:\n# ['RD', 'PC', 'RC', 'RC', 'RC', 'RC', 'RD', 'PC']",
      "metadata": {
        "trusted": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['RD', 'RD', 'RD', 'RD', 'RD', 'RD', 'RD', 'RD']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 3\n\nDado un MDP y una secuencia de estados, valoramos dicha secuencia como la suma de las recompensas de los estados de la secuencias (cada una con el correspondiente descuento). Se pide definir una funci√≥n valora_secuencia(seq,mdp) que calcula esta valoraci√≥n.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Soluci√≥n\n\ndef valora_secuencia(seq, mdp):\n    return sum(mdp.R(e)*(mdp.descuento**i) for i,e in enumerate(seq))\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "valora_secuencia(['PC', 'RC', 'RC', 'RC', 'RC', 'RC', \n                       'RD', 'RD', 'RD', 'PD', 'PD', 'PD', \n                       'PD', 'PD', 'PD', 'PD', 'PD', 'PD', \n                       'PD', 'PD'],mdp_ryc)\n\n# Resultado:\n# 51.2579511",
      "metadata": {
        "trusted": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "51.2579511"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "valora_secuencia(['RD', 'PC', 'PD', 'PC', 'RC', 'RC', \n                        'RD', 'PD', 'PD', 'PC', 'RC', 'RC', \n                        'RC', 'RC', 'RC', 'RC'],mdp_ryc)\n\n# Resultado:\n# 44.11795212148159",
      "metadata": {
        "trusted": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "44.11795212148159"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 4\n\nDada una pol√≠tica pi, la valoraci√≥n de un estado e respecto de esa pol√≠tica, V^{pi}(e), se define como la media esperada de las valoraciones de las secuencias que se pueden generar teniendo dicho estado como estado de partida. Usando las funciones de los dos ejercicios anteriores, definir una funci√≥n \"estima_valor(e,pi,mdp,m,n)\" que realiza una estimaci√≥n aproximada del valor V^{pi}(e), para ello genera n secuencias de tama√±o m, y calcula la media de sus valoraciones.  ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ya que vamos a calcular la media como soluci√≥n principal usaremos el sum(...)/n. Siendo n el n√∫mero de secuencias, y siendo estas las que se van a recorrer valor√°ndolas. \n\nPara ello, como se comenta anteriormente usaremos la funcion de valora_secuencia(...) para valorar las secuencias n veces, como valor de entrada a la funci√≥n le vamos a pasar secuencias que generaremos con la funci√≥n genera_secuencia_estados(...) y pas√°ndole el MDP como entrada.\n\nEn genera_secuencia_estados(...) tenemos al objeto mdp, a e que son los estados de la secuencia, pi que ser√° la pol√≠tica usada y a m la longitud de los estados.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Soluci√≥n:\ndef estima_valor(e,pi,mdp,m,n):\n    return(sum(valora_secuencia(genera_secuencia_estados(mdp,pi,e,m), mdp) for _ in range (n)))/ n\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"PC\",pi_ryc_ahorra,mdp_ryc,50,500)\n\n# Respuesta posible:\n# 14.531471247172597",
      "metadata": {
        "trusted": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"PC\",pi_ryc_2,mdp_ryc,50,500)\n\n# Respuesta posible:\n# 35.92126959549151",
      "metadata": {
        "trusted": true
      },
      "execution_count": 18,
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"RC\",pi_ryc_ahorra,mdp_ryc,60,700)\n\n# Respuesta posible:\n# 32.807558694112984",
      "metadata": {
        "trusted": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99.8202989700079"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"RC\",pi_ryc_2,mdp_ryc,60,700)\n\n# Respuesta posible:\n# 50.09728516585913",
      "metadata": {
        "trusted": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "execution_count": 20,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99.8202989700079"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 5\n\nUsando la funci√≥n anterior, estimar la valoraci√≥n de cada estado del problema \"Rica y conocida\", con las dos siguientes pol√≠ticas:\n\n* Aquella que sea cual sea el estado, siempre decide invertir en publicidad. \n* Aquella que sea cual sea el estado, siempre decide ahorrar. \n\n¬øCu√°l crees que es mejor? ¬øHabr√° alguna mejor que estas dos? ¬øCu√°l crees que ser√≠a la mejor pol√≠tica de todas? ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "La soluci√≥n para este ejercicio no va a ser m√°s que crear las variables que implemente las casu√≠sticas que nos plantean.\n\nA su vez pondremos como nombre a las variables las que nos ofrecen los ejemplos de debajo de la soluci√≥n para que no haya problemas al ejecutarlo.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Soluci√≥n\n\n# Esta es la opci√≥n en la que siempre se invierte en publicidad, refrescamos los estados y acciones en la definici√≥n de RYC si lo necesitamos\npi_ryc_gastar = {\"RC\": \"Gastar en publicidad\",\n                \"RD\": \"Gastar en publicidad\",\n                \"PC\": \"Gastar en publicidad\",\n                \"PD\": \"Gastar en publicidad\"}\n\npi_ryc_ahorra = {\"RC\": \"No publicidad\",\n                \"RD\": \"No publicidad\",\n                \"PC\": \"No publicidad\",\n                \"PD\": \"No publicidad\"}",
      "metadata": {
        "trusted": true
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"RC\",pi_ryc_gastar,mdp_ryc,60,1000)\n\n# Respuesta: 10.0",
      "metadata": {
        "trusted": true
      },
      "execution_count": 30,
      "outputs": [
        {
          "execution_count": 30,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99.8202989700078"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"RC\",pi_ryc_ahorra,mdp_ryc,60,1000)\n\n# Respuesta: 33.354461818277635",
      "metadata": {
        "trusted": true
      },
      "execution_count": 31,
      "outputs": [
        {
          "execution_count": 31,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99.8202989700078"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"RD\",pi_ryc_gastar,mdp_ryc,60,1000)\n\n# Respuesta: 10.0",
      "metadata": {
        "trusted": true
      },
      "execution_count": 32,
      "outputs": [
        {
          "execution_count": 32,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99.8202989700078"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"RD\",pi_ryc_ahorra,mdp_ryc,60,1000)\n\n# Respuesta:18.17532275274187",
      "metadata": {
        "trusted": true
      },
      "execution_count": 33,
      "outputs": [
        {
          "execution_count": 33,
          "output_type": "execute_result",
          "data": {
            "text/plain": "99.8202989700078"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"PC\",pi_ryc_gastar,mdp_ryc,60,1000)\n\n# Respuesta: 0.0",
      "metadata": {
        "trusted": true
      },
      "execution_count": 34,
      "outputs": [
        {
          "execution_count": 34,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"PC\",pi_ryc_ahorra,mdp_ryc,60,1000)\n\n# Respuesta: estima_valor(\"PC\",pi_ryc_ahorra,mdp_ryc,60,1000)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 35,
      "outputs": [
        {
          "execution_count": 35,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "estima_valor(\"PD\",pi_ryc_gastar,mdp_ryc,60,1000)\n\n# Respuesta: 0.0",
      "metadata": {
        "trusted": true
      },
      "execution_count": 36,
      "outputs": [
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Se puede comprobar que salvo en el √∫ltimo caso, que la valoraci√≥n es igual, las valoraciones que se consiguen con la segunda pol√≠tica son mayores que con la primera pol√≠tica. \n\nNinguna de las dos pol√≠ticas es la √≥ptima, como se ver√° m√°s adelante.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 6\n\nLa funci√≥n de valoraci√≥n no se suele calcular mediante la t√©cnica de muestreo vista en el ejercicio 4, sino como resultado de resolver un sistema de ecuaciones. Dicho sistema de ecuaciones se puede resolver de manera proximada de manera iterativa, tal como se explica en el tema.\n\nDefinir una funci√≥n \"valoraci√≥n_respecto_pol√≠tica(pi,mdp, n)\" que aplica dicho m√©todo iterativo (n iteraciones) para calcular la valoraci√≥n V^{pi}. Dicha valoraci√≥n debe devolverse como un diccionario que a cada estado e le asocia el valor \"V^{pi}(e)\" calculado.  \n\nAplicar la funci√≥n para calcular la valoraci√≥n asociada a las dos pol√≠ticas que se dan en el ejercicio anterior, y comparara los valores obtenidos con los obtenidos mediante muestreo. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Soluci√≥n:\n\ndef valoraci√≥n_respecto_pol√≠tica(pi,mdp,n):\n    \n    # Asignamos las variables que vamos a usar en la funci√≥n con respecto a los valores de la clase MDP\n    recompensa, transicion, descuento = mdp.R, mdp.T, mdp.descuento\n    \n    # valoracion va a ser la variable diccionario al que que se le recorre todos los estados y se le asocia el valor calculado\n    valoracion = {e:0 for estado in mdp.estados}\n    \n    for _ in range(n):\n        # Hacemos una copia de valoracion para no modificarla\n        valoracion_copia = valoracion.copy()\n        for estados in mdp.estados:\n            valoracion[estados] = recompensa(estados) + descuento*(sum([pi*valoracion_copia[estados, pi] in transicion(estados, pi[estados])]))\n    \n    return valoracion\n    ",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Calculamos con esta funci√≥n la valoraci√≥n de las dos pol√≠ticas anteriores.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "valoraci√≥n_respecto_pol√≠tica(pi_ryc_gastar,mdp_ryc, 100)\n\n# Resultado:\n# {'RC': 10.0, 'RD': 10.0, 'PC': 0.0, 'PD': 0.0}",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'pi_ryc_gastar' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m valoraci√≥n_respecto_pol√≠tica(\u001b[43mpi_ryc_gastar\u001b[49m,mdp_ryc, \u001b[38;5;241m100\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pi_ryc_gastar' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "valoraci√≥n_respecto_pol√≠tica(pi_ryc_ahorra,mdp_ryc, 100)\n\n# Resultado:\n# {'RC': 33.05785123966942,\n#  'RD': 18.18181818181818,\n#  'PC': 14.876033057851238,\n#  'PD': 0.0}",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'pi_ryc_ahorra' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m valoraci√≥n_respecto_pol√≠tica(\u001b[43mpi_ryc_ahorra\u001b[49m,mdp_ryc, \u001b[38;5;241m100\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pi_ryc_ahorra' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Ejercicio 7\n\nEn el tema 3 se ha visto que la valoraci√≥n de un estado se define como la mejor valoraci√≥n que pueda tener el estado, respecto a todas las pol√≠ticas posibles. Y la pol√≠tica √≥ptima es aquella que en cada estado realiza la acci√≥n con la mejor valoraci√≥n esperada (entendiendo por valoraci√≥n esperada la suma de las valoraciones de los estados que podr√≠an resultar al aplicar dicha acci√≥n, ponderadas por la probabilidad de que ocurra eso). De esta manera, la valoraci√≥n de un estado es la valoraci√≥n que la pol√≠tica √≥ptima asigna al estado.\n\nPara calcular tanto la valoraci√≥n de los estados, como la pol√≠tica √≥ptima, se han visto dos algoritmos: iteraci√≥n de valores e iteraci√≥n de pol√≠ticas. En este ejercicio se pide implementar el algoritmo de iteraci√≥n de pol√≠ticas. En concreto, se pide definir una funci√≥n \"iteraci√≥n_de_pol√≠ticas(mdp,k)\" que implementa el algoritmo de iteraci√≥n de pol√≠ticas, y devuelve dos diccionarios, uno con la valoraci√≥n de los estados y otro con la pol√≠tica √≥ptima. \n\nComparar los resultados obtenidos con las pol√≠ticas del ejercicio 5 y las valoraciones obtenidas.  ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Vamos a definir la funci√≥n que nos devuelva la acci√≥n con la valoraci√≥n m√°xima recorriendo todos los valores que haya en la secuencia de entrada",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def secuencia_maxima(secuencia, funcion):\n    max = float(\"-inf\")\n    #Lo inicializamos a None para que sea un valor neutro\n    valor_max = None\n    for valor in secuencia:\n        funcion_valor = funcion(valor)\n        if funcion_valor > max:\n            max = funcion_valor\n            accion_max = valor\n    return accion_max",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def valoracion_esperada(accion,estado,V,mdp):\n    return sum((p*V[e] for (e,p) in mdp.T(estado,accion)))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Soluci√≥n:\n# Le ponemos la tilde que luego el ejemplo sino da fallo\ndef iteraci√≥n_de_pol√≠ticas(mdp,k):\n    V = {e:0 for e in mdp.estados}\n    politica = {e:random.choice(mdp.A(e)) for e in mdp.estados}\n    \n    while True:\n        V = valoraci√≥n_respecto_pol√≠tica(politica, mdp, k)\n        actualizado = False\n        for e in mdp.estados:\n             accion = secuencia_maxima(mdp.A(e))\n            if(accion != politica[e] and a:valoracion_esperada(accion,estado,V,mdp) > valoracion_esperada(politica[e], e, V, mdp)):\n                politica[e] = accion\n                actualizado = True\n        if not actualizado:\n            return politica, V",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Calculamos la mejor pol√≠tica y su valoraci√≥n con el MDP de Rica_y_Conocida. Como se ve, resulta que lo mejor es s√≥lo gastar en publicidad cuando se es pobre y desconocido. Se observa que la valoraci√≥n respecto de esa pol√≠tica es mejor, que las valoraciones con las pol√≠tica que se vieron en los ejercicios 5 y 6.   ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "iteraci√≥n_de_pol√≠ticas(mdp_ryc,100)\n\n# Respuesta\n# ({'RC': 'No publicidad',\n#   'RD': 'No publicidad',\n#   'PC': 'No publicidad',\n#   'PD': 'Gastar en publicidad'},\n#  {'RC': 54.20053629623792,\n#   'RD': 44.02311379672535,\n#   'PC': 38.602953921506,\n#   'PD': 31.584041852876634})",
      "metadata": {
        "trusted": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'iteraci√≥n_de_pol√≠ticas' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43miteraci√≥n_de_pol√≠ticas\u001b[49m(mdp_ryc,\u001b[38;5;241m100\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'iteraci√≥n_de_pol√≠ticas' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
