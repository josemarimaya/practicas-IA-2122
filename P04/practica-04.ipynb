{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pr谩ctica 4 - Inteligencia Artificial\n",
        "\n",
        "### Grado Ingenier铆a Inform谩tica Tecnolog铆as Inform谩ticas - Curso 2021-22\n",
        "\n",
        "### T茅cnicas metaheur铆sticas para optimizaci贸n\n",
        "### Procesos de Decisi贸n de Markov\n",
        "\n",
        "Jos茅 Luis Ru铆z Reina y comentado por tu chavalito pesao de confianza "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta pr谩ctica vamos a implementar algoritmos relacionados con Procesos de Decisi贸n de Markov. \n",
        "\n",
        "Supondremos que un Procesos de Decisi贸n de Markov (MDP, por sus siglas en ingl茅s) va a ser un objeto de la siguiente clase (o mejor dicho, de una subclase de la siguiente clase). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class MDP(object):\n",
        "\n",
        "    \"\"\"La clase gen茅rica MDP tiene como m茅todos la funci贸n de recompensa R,\n",
        "    la funci贸n A que da la lista de acciones aplicables a un estado, y la\n",
        "    funci贸n T que implementa el modelo de transici贸n. Para cada estado y\n",
        "    acci贸n aplicable al estado, la funci贸n T devuelve una lista de pares\n",
        "    (ei,pi) que describe los posibles estados ei que se pueden obtener al\n",
        "    plical la acci贸n al estado, junto con la probabilidad pi de que esto\n",
        "    ocurra. El constructor de la clase recibe la lista de estados posibles y\n",
        "    el factor de descuento.\n",
        "\n",
        "    En esta clase gen茅rica, las funciones R, A y T aparecen sin definir. Un\n",
        "    MDP concreto va a ser un objeto de una subclase de esta clase MDP, en la\n",
        "    que se definir谩n de manera concreta estas tres funciones\"\"\"  \n",
        "\n",
        "    def __init__(self,estados,descuento):\n",
        "\n",
        "        self.estados=estados\n",
        "        self.descuento=descuento\n",
        "\n",
        "    def R(self,estado):\n",
        "        pass\n",
        "\n",
        "    def A(self,estado):\n",
        "        pass\n",
        "        \n",
        "    def T(self,estado,accion):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consideramos el siguiente problema:\n",
        "\n",
        "A lo largo de su vida, una empresa pasa por situaciones muy distintas, que por simplificar resumiremos en que al inicio de cada campa帽a puede estar rica o pobre, y ser conocida o desconocida.  Para ello puede decidir en cada momento o bien invertir en publicidad, o bien optar por no hacer publicidad. Estas dos acciones no tienen siempre un resultado fijo, aunque podemos describirlo de manera probabil铆stica:\n",
        "\n",
        "* Si la empresa es rica y conocida y no invierte en publicidad, seguir谩 rica, pero existe un 50% de probabilidad de que se vuelva desconocida. Si gasta en publicidad, con toda seguridad seguir谩 conocida pero pasar谩 a ser pobre.  \n",
        "* Si la empresa es rica y desconocida y no gasta en publicidad, seguir谩 desconocida, y existe un 50% de que se vuelva pobre. Si gasta en publicidad, se volver谩 pobre, pero existe un 50% de probabilidades de que se vuelva conocida.\n",
        "* Si la empresa es pobre y conocida y no invierte en publicidad, pasar谩 a ser pobre y desconocida con un 50% de probabilidad, y rica y conocida en caso contrario. Si gasta en publicidad, con toda seguridad seguir谩 en la misma situaci贸n. \n",
        "* Si la empresa es pobre y desconocida, y no invierte en publicidad, seguir谩 en la misma situaci贸n con toda seguridad. Si gasta en publicidad, seguir谩 pobre, pero con un 50% de posibilidades pasar谩 aser conocida. \n",
        "\n",
        "Supondremos que la recompensa en una campa帽a en la que la empresa es rica es de 10, y de 0 en en las que sea pobre. El objetivo es conseguir la mayor recompesa acumulada a lo largo del tiempo, aunque penalizaremos las ganancias obtenidas en campa帽as muy lejanas en el tiempo, introduciendo un factor de descuento de 0.9. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Ejercicio 1\n",
        "\n",
        "Se pide representar el problema como un proceso de decisi贸n de Markov, definiendo una clase Rica_y_Conocida, subclase de la clase MDP gen茅rica, cuyo constructor recibe como entrada 煤nicamente el factor de descuento, y en la que se definen de manera concreta los m茅todos R, A y T, seg煤n lo descrito. Para ello:\n",
        "* La recompensa la guardaremos en un diccionario con claves \"RC\", \"RD\", \"PC\" y \"PD\", donde \"R\" es Rica, \"P\" es pobre, \"C\" es conocida y \"D\" es desconocida. Los valor del diccionario son las recompensas asociadas.\n",
        "* Las acciones ser谩n: \"No publicidad\" y \"Gastar en publicidad\"\n",
        "* La transici贸n tambi茅n ser谩 un diccionario donde las claves son tuplas (Estado, acci贸n) y los valores son listas de pares (Nuevo_estado, probabilidad). Por ejemplo un par clave-valor del diccionario ser谩 (\"RC\",\"No publicidad\"):[(\"RC\",0.5),(\"RD\",0.5)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recordamos que el factor de descuento es 0.9, que es el que viene definido en el enunciado, adem谩s que la recompensa para ser rica es 10 y por ser pobre es 0. \n",
        "\n",
        "Tambi茅n sabemmos que R es rica, P es pobre, C es conocida y D desconocida. Por tanto tenemos:\n",
        "\n",
        "* RC es Rica y Conocida\n",
        "* RD es Rica y Desconocida\n",
        "* PC es Pobre y Conocida\n",
        "* PD es pobre y Desconocida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Soluci贸n:\n",
        "class Rica_y_Conocida(MDP):\n",
        "    \n",
        "    def __init__(self, factor_descuento = 0.9):\n",
        "        \n",
        "        # Definimos las recompensas por estado, si es rica 10 y si es pobre 0\n",
        "        self.DiccionarioRecompensa = {\"RC\": 10, \"RD\": 10, \"PC\": 0, \"PD\": 0}\n",
        "        self.DiccionarioTransicion = {\n",
        "            # Si la empresa no invierte en publicidad hay un 50% de que siga siendo desconocida y 50% de que se vuelva desconocida. Pero sigue siendo rica\n",
        "            (\"RC\", \"No publicidad\"): [(\"RC\", 0.5), (\"RD\", 0.5)],\n",
        "            # Si se gasta el dinero en publicidad seguro (100%) que se vuelve pobre pero sigue siendo conocida\n",
        "            (\"RC\", \"Gastar en publicidad\"): [(\"PC\", 1)],\n",
        "            # Si no gasta en publicidad seguir谩 siendo desconocida pero con un 50% de que se vuelva pobre o 50% de seguir siendo rica\n",
        "            (\"RD\", \"No publicidad\"): [(\"RD\", 0.5), (\"PD\", 0.5)],\n",
        "            # Si gasta en publicidad se vuelve pobre seguro (100%) pero con un 50% de ser conocida o 50% de seguir siendo desconocida\n",
        "            (\"RD\", \"Gastar en publicidad\"): [(\"PD\", 0.5), (\"PC\", 0.5)],\n",
        "            # Si no invierte en publicidad hay un 50% de volverse pobre y desconocida o un 50% de volverse rica y seguir siendo conocida\n",
        "            (\"PC\", \"No publicidad\"): [(\"PD\", 0.5),(\"RC\", 0.5)],\n",
        "            # Si gasta en publicidad seguir谩 siendo pobre y conocida (100%)\n",
        "            (\"PC\", \"Gastar en publicidad\"): [(\"PC\", 1)],\n",
        "            # Si es pobre y desconocida sin gastar en publicidad con toda posibilidad (100%) va a seguir siendo pobre y desconocida\n",
        "            (\"PD\", \"No publicidad\"): [(\"PD\", 1)],\n",
        "            # Si gasta en publicidad hay un 50% que siga igual o un 50% de que se vuelva pobre y conocida\n",
        "            (\"PD\", \"Gastar en publicidad\"): [(\"PD\", 0.5),(\"PC\", 0.5)]\n",
        "            \n",
        "        }\n",
        "        estados = [\"RC\", \"RD\", \"PC\", \"PD\"]\n",
        "        super().__init__(estados, factor_descuento)\n",
        "        \n",
        "    # Definimos las recompensas\n",
        "    def R(self,estado):\n",
        "        return self.DiccionarioRecompensa[estado]\n",
        "    \n",
        "    # Definimos las acciones\n",
        "    def A(self,estado):\n",
        "        return [\"No publicidad\", \"Gastar en publicidad\"]\n",
        "\n",
        "    # Definimos las transiciones\n",
        "    def T(self,estado,accion):\n",
        "        return self.DiccionarioTransicion[(estado,accion)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 2\n",
        "\n",
        "En general, dado un MDP, representaremos una pol铆tica para el mismo como un diccionario cuyas claves son los estados, y los valores las acciones. Una pol铆tica representa una manera concreta de decidir en cada estado la acci贸n (de entre las aplicables a ese estado) que ha de aplicarse. \n",
        "\n",
        "Dado un MDP, un estado de partida, y una pol铆tica concreta, podemos generar (muestrear) una secuencia de estados por los que ir铆amos pasando si vamos aplicando las acciones que nos indica la pol铆tica: dado un estado de la secuencia, aplicamos a ese estado la acci贸n que indique la pol铆tica, y obtenemos un estado siguiente de manera aleatoria, pero siguiendo la distribuci贸n de probabilidad que indica el modelo de transici贸n dado por el m茅todo T.  \n",
        "\n",
        "Se pide definir una funci贸n \"genera_secuencia_estados(mdp,pi,e,n)\" que devuelva una secuencia de estados de longitud n, obtenida siguiendo el m茅todo anterior. Aqu铆 mdp es objeto de la clase MDP, pi es una pol铆tica, e un estado de partida y n la longitud de la secuencia. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Soluci贸n\n",
        "\n",
        "def muestreo(estado):\n",
        "    # Definimos valor aleatorio que usaremos en el muestreo\n",
        "    valor_aleatorio = random.random()\n",
        "    # Inicializamos la variable que tendr谩 la pol铆tica del estado que habr谩 en el bucle actualmente\n",
        "    politica_actual = 0\n",
        "    # Bucle que recorre el estado con el par accion, politica\n",
        "    for accion,politica in estado:\n",
        "        # A帽adimos la politica actual y vamos a帽adi茅ndola a la variable\n",
        "        politica_actual += politica\n",
        "        # Si la pol铆tica actual es mayor que el valor aleatorio ...\n",
        "        if politica_actual > valor_aleatorio:\n",
        "            # ... devolvemos el valor de la pol铆tica actual\n",
        "            return politica_actual\n",
        "    \n",
        "def genera_secuencia_estados(mdp,pi,e,n):\n",
        "    # Vamos a usar el estado e como entrada\n",
        "    actual = e\n",
        "    # La variable con la secuencia generada recoge el estado_actual\n",
        "    secuencia_generada = [actual]\n",
        "    # Hace el for iterando n-1 porque empezamos por el valor 0\n",
        "    for _ in range(n-1):\n",
        "        # Le pasamos el modelo mdp marcando la transici贸n con el estado y la pol铆tica del mismo\n",
        "        estado_actual = muestreo(mdp.T(actual, pi[actual]))\n",
        "        # A帽adimos el estado del muestreo a la secuencia que queremos generar\n",
        "        secuencia_generada.append(actual)\n",
        "    return secuencia_generada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Este c贸digo da el buen resultado, es mejor que el anterior\n",
        "def muestreo(distr):\n",
        "\n",
        "    r=random.random()\n",
        "    \n",
        "    acum=0\n",
        "    \n",
        "    for v,p in distr:\n",
        "    \n",
        "        acum+=p\n",
        "    \n",
        "        if acum>r:\n",
        "\n",
        "            return v\n",
        "    \n",
        "def genera_secuencia_estados(mdp,pi,e,n):\n",
        "\n",
        "  actual=e\n",
        "\n",
        "  seq=[actual]\n",
        "\n",
        "  for _ in range(n-1):\n",
        "\n",
        "    actual=muestreo(mdp.T(actual,pi[actual]))\n",
        "\n",
        "    seq.append(actual)\n",
        "\n",
        "  return seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos a continuaci贸n algunos ejemplo de uso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Definimos una instancia de la subclase \n",
        "mdp_ryc=Rica_y_Conocida()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['PC', 'PC', 'PC', 'PC', 'PC', 'PC', 'PC', 'PC', 'PC', 'PC']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pi_ryc_ahorra={\"RC\":\"No publicidad\",\"RD\":\"No publicidad\",\n",
        "                    \"PC\":\"No publicidad\",\"PD\":\"No publicidad\"}\n",
        "genera_secuencia_estados(mdp_ryc,pi_ryc_ahorra,\"PC\",10)\n",
        "\n",
        "# Posible resultado:\n",
        "# ['PC', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD', 'PD']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['RD', 'RD', 'RD', 'RD', 'RD', 'RD', 'RD', 'RD']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pi_ryc_2={\"RC\":\"No publicidad\",\"RD\":\"Gastar en publicidad\",\n",
        "               \"PC\":\"No publicidad\",\"PD\":\"Gastar en publicidad\"}\n",
        "genera_secuencia_estados(mdp_ryc,pi_ryc_2,\"RD\",8)\n",
        "\n",
        "# Posible resultado:\n",
        "# ['RD', 'PC', 'RC', 'RC', 'RC', 'RC', 'RD', 'PC']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 3\n",
        "\n",
        "Dado un MDP y una secuencia de estados, valoramos dicha secuencia como la suma de las recompensas de los estados de la secuencias (cada una con el correspondiente descuento). Se pide definir una funci贸n valora_secuencia(seq,mdp) que calcula esta valoraci贸n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Soluci贸n\n",
        "\n",
        "def valora_secuencia(seq, mdp):\n",
        "    return sum(mdp.R(e)*(mdp.descuento**i) for i,e in enumerate(seq))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "51.2579511"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valora_secuencia(['PC', 'RC', 'RC', 'RC', 'RC', 'RC', \n",
        "                       'RD', 'RD', 'RD', 'PD', 'PD', 'PD', \n",
        "                       'PD', 'PD', 'PD', 'PD', 'PD', 'PD', \n",
        "                       'PD', 'PD'],mdp_ryc)\n",
        "\n",
        "# Resultado:\n",
        "# 51.2579511"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "44.11795212148159"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valora_secuencia(['RD', 'PC', 'PD', 'PC', 'RC', 'RC', \n",
        "                        'RD', 'PD', 'PD', 'PC', 'RC', 'RC', \n",
        "                        'RC', 'RC', 'RC', 'RC'],mdp_ryc)\n",
        "\n",
        "# Resultado:\n",
        "# 44.11795212148159"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 4\n",
        "\n",
        "Dada una pol铆tica pi, la valoraci贸n de un estado e respecto de esa pol铆tica, V^{pi}(e), se define como la media esperada de las valoraciones de las secuencias que se pueden generar teniendo dicho estado como estado de partida. Usando las funciones de los dos ejercicios anteriores, definir una funci贸n \"estima_valor(e,pi,mdp,m,n)\" que realiza una estimaci贸n aproximada del valor V^{pi}(e), para ello genera n secuencias de tama帽o m, y calcula la media de sus valoraciones.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ya que vamos a calcular la media como soluci贸n principal usaremos el sum(...)/n. Siendo n el n煤mero de secuencias, y siendo estas las que se van a recorrer valor谩ndolas. \n",
        "\n",
        "Para ello, como se comenta anteriormente usaremos la funcion de valora_secuencia(...) para valorar las secuencias n veces, como valor de entrada a la funci贸n le vamos a pasar secuencias que generaremos con la funci贸n genera_secuencia_estados(...) y pas谩ndole el MDP como entrada.\n",
        "\n",
        "En genera_secuencia_estados(...) tenemos al objeto mdp, a e que son los estados de la secuencia, pi que ser谩 la pol铆tica usada y a m la longitud de los estados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Soluci贸n:\n",
        "def estima_valor(e,pi,mdp,m,n):\n",
        "    return(sum(valora_secuencia(genera_secuencia_estados(mdp,pi,e,m), mdp) for _ in range (n)))/ n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estima_valor(\"PC\",pi_ryc_ahorra,mdp_ryc,50,500)\n",
        "\n",
        "# Respuesta posible:\n",
        "# 14.531471247172597"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estima_valor(\"PC\",pi_ryc_2,mdp_ryc,50,500)\n",
        "\n",
        "# Respuesta posible:\n",
        "# 35.92126959549151"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "99.8202989700079"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estima_valor(\"RC\",pi_ryc_ahorra,mdp_ryc,60,700)\n",
        "\n",
        "# Respuesta posible:\n",
        "# 32.807558694112984"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "99.8202989700079"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estima_valor(\"RC\",pi_ryc_2,mdp_ryc,60,700)\n",
        "\n",
        "# Respuesta posible:\n",
        "# 50.09728516585913"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 5\n",
        "\n",
        "Usando la funci贸n anterior, estimar la valoraci贸n de cada estado del problema \"Rica y conocida\", con las dos siguientes pol铆ticas:\n",
        "\n",
        "* Aquella que sea cual sea el estado, siempre decide invertir en publicidad. \n",
        "* Aquella que sea cual sea el estado, siempre decide ahorrar. \n",
        "\n",
        "驴Cu谩l crees que es mejor? 驴Habr谩 alguna mejor que estas dos? 驴Cu谩l crees que ser铆a la mejor pol铆tica de todas? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La soluci贸n para este ejercicio no va a ser m谩s que crear las variables que implemente las casu铆sticas que nos plantean.\n",
        "\n",
        "A su vez pondremos como nombre a las variables las que nos ofrecen los ejemplos de debajo de la soluci贸n para que no haya problemas al ejecutarlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Soluci贸n\n",
        "\n",
        "# Esta es la opci贸n en la que siempre se invierte en publicidad, refrescamos los estados y acciones en la definici贸n de RYC si lo necesitamos\n",
        "pi_ryc_gastar = {\"RC\": \"Gastar en publicidad\",\n",
        "                \"RD\": \"Gastar en publicidad\",\n",
        "                \"PC\": \"Gastar en publicidad\",\n",
        "                \"PD\": \"Gastar en publicidad\"}\n",
        "\n",
        "pi_ryc_ahorra = {\"RC\": \"No publicidad\",\n",
        "                \"RD\": \"No publicidad\",\n",
        "                \"PC\": \"No publicidad\",\n",
        "                \"PD\": \"No publicidad\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "99.8202989700078"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estima_valor(\"RC\",pi_ryc_gastar,mdp_ryc,60,1000)\n",
        "\n",
        "# Respuesta: 10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "99.8202989700078"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estima_valor(\"RC\",pi_ryc_ahorra,mdp_ryc,60,1000)\n",
        "\n",
        "# Respuesta: 33.354461818277635"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "99.8202989700078"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estima_valor(\"RD\",pi_ryc_gastar,mdp_ryc,60,1000)\n",
        "\n",
        "# Respuesta: 10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "99.8202989700078"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estima_valor(\"RD\",pi_ryc_ahorra,mdp_ryc,60,1000)\n",
        "\n",
        "# Respuesta:18.17532275274187"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estima_valor(\"PC\",pi_ryc_gastar,mdp_ryc,60,1000)\n",
        "\n",
        "# Respuesta: 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estima_valor(\"PC\",pi_ryc_ahorra,mdp_ryc,60,1000)\n",
        "\n",
        "# Respuesta: estima_valor(\"PC\",pi_ryc_ahorra,mdp_ryc,60,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "estima_valor(\"PD\",pi_ryc_gastar,mdp_ryc,60,1000)\n",
        "\n",
        "# Respuesta: 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se puede comprobar que salvo en el 煤ltimo caso, que la valoraci贸n es igual, las valoraciones que se consiguen con la segunda pol铆tica son mayores que con la primera pol铆tica. \n",
        "\n",
        "Ninguna de las dos pol铆ticas es la 贸ptima, como se ver谩 m谩s adelante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 6\n",
        "\n",
        "La funci贸n de valoraci贸n no se suele calcular mediante la t茅cnica de muestreo vista en el ejercicio 4, sino como resultado de resolver un sistema de ecuaciones. Dicho sistema de ecuaciones se puede resolver de manera proximada de manera iterativa, tal como se explica en el tema.\n",
        "\n",
        "Definir una funci贸n \"valoraci贸n_respecto_pol铆tica(pi,mdp, n)\" que aplica dicho m茅todo iterativo (n iteraciones) para calcular la valoraci贸n V^{pi}. Dicha valoraci贸n debe devolverse como un diccionario que a cada estado e le asocia el valor \"V^{pi}(e)\" calculado.  \n",
        "\n",
        "Aplicar la funci贸n para calcular la valoraci贸n asociada a las dos pol铆ticas que se dan en el ejercicio anterior, y comparara los valores obtenidos con los obtenidos mediante muestreo. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Soluci贸n:\n",
        "\n",
        "def valoraci贸n_respecto_pol铆tica(pi,mdp,n):\n",
        "    \n",
        "    # Asignamos las variables que vamos a usar en la funci贸n con respecto a los valores de la clase MDP\n",
        "    recompensa, transicion, descuento = mdp.R, mdp.T, mdp.descuento\n",
        "    \n",
        "    # valoracion va a ser la variable diccionario al que que se le recorre todos los estados y se le asocia el valor calculado\n",
        "    valoracion = {e:0 for estado in mdp.estados}\n",
        "    \n",
        "    for _ in range(n):\n",
        "        # Hacemos una copia de valoracion para no modificarla\n",
        "        valoracion_copia = valoracion.copy()\n",
        "        for estados in mdp.estados:\n",
        "            valoracion[estados] = recompensa(estados) + descuento*(sum([pi*valoracion_copia[estados, pi] in transicion(estados, pi[estados])]))\n",
        "    \n",
        "    return valoracion\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C贸digo bueno con respecto al anterior\n",
        "def valoraci贸n_respecto_pol铆tica(pi,mdp, k):\n",
        "\n",
        "  \"\"\"Calcula una aproximaci贸n a la valoraci贸n de los estados respecto de la\n",
        "\n",
        "  pol铆tica pi, aplicando el m茅todo iterativo\"\"\"\n",
        "\n",
        "  R, T, gamma = mdp.R, mdp.T, mdp.descuento\n",
        "\n",
        "  V = {e:0 for e in mdp.estados}\n",
        "\n",
        "  for _ in range(k):\n",
        "\n",
        "    V1 = V.copy()\n",
        "\n",
        "    for s in mdp.estados:\n",
        "\n",
        "      #acum = 0\n",
        "\n",
        "      #for (s1,p) in T(s, pi[s]):\n",
        "\n",
        "      #  acum += p * V1[s1]\n",
        "\n",
        "      #V[s] = R(s) + gamma * acum\n",
        "\n",
        "      V[s] = R(s) + gamma *(sum([p * V1[s1] for (s1,p) in T(s, pi[s])]))\n",
        "\n",
        "  return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculamos con esta funci贸n la valoraci贸n de las dos pol铆ticas anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'pi_ryc_gastar' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m valoraci贸n_respecto_pol铆tica(\u001b[43mpi_ryc_gastar\u001b[49m,mdp_ryc, \u001b[38;5;241m100\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pi_ryc_gastar' is not defined"
          ]
        }
      ],
      "source": [
        "valoraci贸n_respecto_pol铆tica(pi_ryc_gastar,mdp_ryc, 100)\n",
        "\n",
        "# Resultado:\n",
        "# {'RC': 10.0, 'RD': 10.0, 'PC': 0.0, 'PD': 0.0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'pi_ryc_ahorra' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m valoraci贸n_respecto_pol铆tica(\u001b[43mpi_ryc_ahorra\u001b[49m,mdp_ryc, \u001b[38;5;241m100\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pi_ryc_ahorra' is not defined"
          ]
        }
      ],
      "source": [
        "valoraci贸n_respecto_pol铆tica(pi_ryc_ahorra,mdp_ryc, 100)\n",
        "\n",
        "# Resultado:\n",
        "# {'RC': 33.05785123966942,\n",
        "#  'RD': 18.18181818181818,\n",
        "#  'PC': 14.876033057851238,\n",
        "#  'PD': 0.0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejercicio 7\n",
        "\n",
        "En el tema 3 se ha visto que la valoraci贸n de un estado se define como la mejor valoraci贸n que pueda tener el estado, respecto a todas las pol铆ticas posibles. Y la pol铆tica 贸ptima es aquella que en cada estado realiza la acci贸n con la mejor valoraci贸n esperada (entendiendo por valoraci贸n esperada la suma de las valoraciones de los estados que podr铆an resultar al aplicar dicha acci贸n, ponderadas por la probabilidad de que ocurra eso). De esta manera, la valoraci贸n de un estado es la valoraci贸n que la pol铆tica 贸ptima asigna al estado.\n",
        "\n",
        "Para calcular tanto la valoraci贸n de los estados, como la pol铆tica 贸ptima, se han visto dos algoritmos: iteraci贸n de valores e iteraci贸n de pol铆ticas. En este ejercicio se pide implementar el algoritmo de iteraci贸n de pol铆ticas. En concreto, se pide definir una funci贸n \"iteraci贸n_de_pol铆ticas(mdp,k)\" que implementa el algoritmo de iteraci贸n de pol铆ticas, y devuelve dos diccionarios, uno con la valoraci贸n de los estados y otro con la pol铆tica 贸ptima. \n",
        "\n",
        "Comparar los resultados obtenidos con las pol铆ticas del ejercicio 5 y las valoraciones obtenidas.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a definir la funci贸n que nos devuelva la acci贸n con la valoraci贸n m谩xima recorriendo todos los valores que haya en la secuencia de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Soluci贸n:\n",
        "\n",
        "def argmax(seq,f):\n",
        "    max=float(\"-inf\")\n",
        "    amax=None\n",
        "    for x in seq:\n",
        "        fx=f(x)\n",
        "        if fx>max:\n",
        "            max=fx\n",
        "            amax=x\n",
        "    return amax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def valoraci贸n_esperada(acc,estado,V,mdp):\n",
        "    \"\"\" Encuentra la valoraci贸n esperada de una acci贸n respecto de una funci贸n\n",
        "    de valoraci贸n V\"\"\"\n",
        "\n",
        "    return sum((p * V[e] for (e,p) in mdp.T(estado, acc)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def iteraci贸n_de_pol铆ticas(mdp,k):\n",
        "    \"Algoritmo de iteraci贸n de pol铆ticas\"\n",
        "    V = {e:0 for e in mdp.estados}\n",
        "    pi = {e:random.choice(mdp.A(e)) for e in mdp.estados}\n",
        "    while True:\n",
        "        V = valoraci贸n_respecto_pol铆tica(pi,mdp, k)\n",
        "        actualizado = False\n",
        "        for e in mdp.estados:\n",
        "            acc = argmax(mdp.A(e), lambda a:valoraci贸n_esperada(a, e,V, mdp))\n",
        "            if (acc != pi[e] and \n",
        "                valoraci贸n_esperada(acc, e,V, mdp) > valoraci贸n_esperada(pi[e], e,V, mdp)): # Por si hay empate\n",
        "                pi[e] = acc\n",
        "                actualizado = True\n",
        "        if not actualizado:\n",
        "            return pi,V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculamos la mejor pol铆tica y su valoraci贸n con el MDP de Rica_y_Conocida. Como se ve, resulta que lo mejor es s贸lo gastar en publicidad cuando se es pobre y desconocido. Se observa que la valoraci贸n respecto de esa pol铆tica es mejor, que las valoraciones con las pol铆tica que se vieron en los ejercicios 5 y 6.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'iteraci贸n_de_pol铆ticas' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43miteraci贸n_de_pol铆ticas\u001b[49m(mdp_ryc,\u001b[38;5;241m100\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'iteraci贸n_de_pol铆ticas' is not defined"
          ]
        }
      ],
      "source": [
        "iteraci贸n_de_pol铆ticas(mdp_ryc,100)\n",
        "\n",
        "# Respuesta\n",
        "# ({'RC': 'No publicidad',\n",
        "#   'RD': 'No publicidad',\n",
        "#   'PC': 'No publicidad',\n",
        "#   'PD': 'Gastar en publicidad'},\n",
        "#  {'RC': 54.20053629623792,\n",
        "#   'RD': 44.02311379672535,\n",
        "#   'PC': 38.602953921506,\n",
        "#   'PD': 31.584041852876634})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
